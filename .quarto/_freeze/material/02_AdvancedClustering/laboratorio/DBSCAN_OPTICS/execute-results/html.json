{
  "hash": "83ad19caa4422b0a2a9501a5dd9b6aaa",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"DBSCAN i OPTICS\"\nauthor: Dante Conti, Sergi Ramirez, (c) IDEAI\ndate: \"2025-01-11\"\ndate-modified: \"2025-01-11\"\ntoc: true\n# language: es\nnumber-sections: true\nformat: \n  html: \n    theme:\n      light: cerulean\n      dark: darkly\neditor: visual\n#execute: \n#  freeze: auto\n---\n\n\n\n# Descripció del problema\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Cargamos las librerias necesarias\nlibrary(cluster)\nlibrary(fpc)\nlibrary(pracma)\nlibrary(factoextra)\nlibrary(dbscan)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n### Generamos una semilla para poder ejecutar los datos\nset.seed(04102022)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n### Creamos la base de datos que vamos a utilizar para detectar los grupos\ndata(\"multishapes\")\ndatos <- multishapes[, 1:2]\n\n### Printamos la imagen que hemos obtenido de los datos a clasificar\nggplot2::ggplot(datos, aes(x = x, y = y)) + \n  ggplot2::geom_point(color='#3333FF')\n```\n\n::: {.cell-output-display}\n![](DBSCAN_OPTICS_files/figure-html/carregar-dades-1.png){width=672}\n:::\n:::\n\n\n\n###  KMeans\n\nGraficamos los datos a través de un k-means para visualizar como quedarian los grupos cuando utilizamos unos algoritmos de agrupación a partir de la inercia.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkm_clusters <- kmeans(x = datos, centers = 5, nstart = 50)\nfviz_cluster(object = km_clusters, data = datos, geom = \"point\", ellipse = FALSE,\n             show.clust.cent = FALSE, pallete = \"jco\") +\n  theme_bw() +\n  theme(legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![](DBSCAN_OPTICS_files/figure-html/kmeans-dades-1.png){width=672}\n:::\n:::\n\n\n\nComo podemos ver, Kmeans ha hecho una muy mala clusterización, puesto que:\n\n- No ha conseguido clusterizar según las formas complejas del modelo.\n\n- No ha tenido en cuenta que existen outliers, incluyendolos en los distintos clusters\n\n### DBSCAN\n\nDBSCAN parte de dos parámetros que son: \n\n- eps: distancia máxima a la que debe haber otra observación para ser considerar que cumple con el criterio de *\"estar cerca\"*\n\n- `minPts`: paámetro que controla la densidad mínima requerida para que un punto sea considerado un núcleo y se incluya en un grupo/cluster.\n\nPara un punto $p$, si existen al menos `minPts` puntos dentro del radio `eps` alrededor de $p$, entonces $p$ se considera un núcleo (*core point*) y se incluye en el mismo grupo/clúster que los demás puntos dentro del radio `eps`. \n\nSi no hay suficientes puntos dentro del radio `eps`, $p$ se considera un punto frontera (*border point*) y se incluye en el mismo grupo/clúster que su punto núcleo más cercano. \n\nSi no hay ningún punto dentro del radio `eps`, $p$ se considera un punto de ruido (*noise point*) y no se incluye en ningún grupo/cluster.\n\nAplicamos el algoritmo de dbscan para classificar los datos.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndbscan_res <- dbscan::dbscan(datos, eps = 0.15, minPts = 5)\n```\n:::\n\n\n\nGraficamos el dbscan obtenido \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfviz_cluster(object = dbscan_res, data = datos, geom = \"point\", ellipse = FALSE,\n             show.clust.cent = FALSE, pallete = \"jco\") +\n  theme_bw() +\n  theme(legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![](DBSCAN_OPTICS_files/figure-html/visualizar-dbscan-1.png){width=672}\n:::\n:::\n\n\n\nPara escoger los valores de `eps` i `minPts`, necesitaremos optimizar el proceso. Para ello, realizaremos la siguiente técnica de optimización. \n\n# Cálculo de min_pts\n\nEl parámetro `min_pts` establece el número de puntos mínimo que, dado un radio `eps`, tiene que haber para que se considere que dichos puntos forman un cluster.\n\nUn valor bajo de `min_pts` asegurará que mas puntos son agrupados, pero se corre el riesgo de agrupar outliers. Por el contrario, un valor muy alto de `min_pts` puede descartar valores que no son anómalos.\n\n\nEn la literatura hablan de usar un valor entre 3 y 5 ya que funcionan bastante bien en la mayoría de los casos. `minPts` igual 2 cuando tenemos una distribución normal y otra nube de outliers\n\nPara calcularlo de manera empírica, diremos que el mínimo de puntos sea igual al 0.2% - 0.25% del total de los datos teniendo en cuenta que: \n\n- El minimo será de 2 para datos que sean muy pequeños\n\n- El máximo será de 10 para datos con mucha información o quizás\n\n- un poco más dependiendo del tamaño de la base de datos\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nporcentaje <- 0.0025 \n\n# Cálculo de min_pts. \nmin_pts <- round(nrow(datos) * porcentaje) \nmin_pts\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 3\n```\n\n\n:::\n\n```{.r .cell-code}\n# Realizamos los cortes de 2 y 10 que se mencionan anteriormente como validación\n# adicional, pero lineas 98 y 99 pueden comentarse.\nmin_pts <- ifelse(min_pts <= 1, 2, min_pts)\nmin_pts <- ifelse(min_pts >= 10, 10, min_pts)\nmin_pts \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 3\n```\n\n\n:::\n:::\n\n\n\n## Normalización de los datos \n\nEste proceso **siempre se ha de realizar** para que cualquier variable tenga el mismo peso delante del cálculo del *clustering*. \n\nCuando trabajamos con distáncias es aconsejable normalizar los datos para que ninguno tenga un peso que no le corresponde.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndatos_norm <- data.frame(lapply(datos, scales::rescale))\nhead(datos_norm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          x         y\n1 0.2299066 0.5427018\n2 0.7855506 0.8076425\n3 0.8104815 0.6681865\n4 0.2470507 0.6168200\n5 0.7365786 0.9038067\n6 0.8465449 0.8135107\n```\n\n\n:::\n\n```{.r .cell-code}\nggplot2::ggplot(datos_norm, aes(x = x, y = y)) + \n  ggplot2::geom_point(color='#3333FF')\n```\n\n::: {.cell-output-display}\n![](DBSCAN_OPTICS_files/figure-html/normalizacion2-1.png){width=672}\n:::\n:::\n\n\n\nComo podemos ver, ahora tendremos los valores entre el intervalo [0, 1]. \n\n## Bibliografia\n\n-   Beautiful dendrogram visualizations in r: 5+ must known methods - unsupervised machine learning - easy guides - wiki - sthda. (s.f.). Recuperado 11 de enero de 2025, de <https://www.sthda.com/english/wiki/beautiful-dendrogram-visualizations-in-r-5-must-known-methods-unsupervised-machine-learning?title=beautiful-dendrogram-visualizations-in-r-5-must-known-methods-unsupervised-machine-learning>\n\n-   Dendrograms. (s.f.). Recuperado 11 de enero de 2025, de <https://plotly.com/ggplot2/dendrogram/>\n\n-   Introduction. (s.f.). \\[Software\\]. Recuperado 11 de enero de 2025, de <https://cran.r-project.org/web/packages/dendextend/vignettes/dendextend.html>\n\n-   RPubs—Clustering de series temporales a traves de metodos no parametricos. (s.f.). Recuperado 11 de enero de 2025, de <https://rpubs.com/Edison-D/615477>\n\n-   RPubs—Dinamyc time warping. (s.f.). Recuperado 11 de enero de 2025, de <https://rpubs.com/sebas_Alf/684217>\n\n-   Sardá, A. (2024). Asardaes/dtwclust \\[R\\]. <https://github.com/asardaes/dtwclust> (Obra original publicada en 2015)\n",
    "supporting": [
      "DBSCAN_OPTICS_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}