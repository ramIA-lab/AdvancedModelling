clases <- sapply(dades, class)
varCat <- names(clases)[which(clases %in% c("character", "factor"))]
for (vC in varCat) {
dades %>%
count(get(vC)) %>% print()
}
dades <- dades %>%
select(-transaction_date, -custom_location)
colnames(dades)
dades <- dades %>%
select(-transaction_date, -customer_location)
head(dades)
table(dades$device_used)
#| label: eliminar-variables
#| echo: true
#| eval: true
#| warning: false
#| message: false
#| error: false
dades <- dades %>%
mutate(reagrupat = case_when(
device_used == "desktop" ~ "sobretaula",
device_used %in% c("mobile", "tablet") ~ "tàctil",
.default = device_used
))
head(dades)
#| label: eliminar-variables
#| echo: true
#| eval: true
#| warning: false
#| message: false
#| error: false
dades <- dades %>%
mutate(reagrupat = case_when(
is_fraudulent == 0 ~ "no",
is_fraudulent == 1 ~ "si",
.default = is_fraudulent
))
#| label: eliminar-variables
#| echo: true
#| eval: true
#| warning: false
#| message: false
#| error: false
dades <- dades %>%
mutate(is_fraudulent = case_when(
is_fraudulent == 0 ~ "no",
is_fraudulent == 1 ~ "si",
.default = is_fraudulent
))
#| label: carregar-dades
#| echo: true
#| eval: true
#| warning: false
#| message: false
#| error: false
## Ruta ón es troba les nostres dades
path <- "./"
## Nom del fitxer de les dades
fitxer <- ""
## Carreguem la base de dades
dades <- read.csv(paste0(path, "Fraudulent_E-Commerce_Transaction_Data.csv"))
#| label: selecció-variables
#| echo: true
#| eval: true
#| warning: false
#| message: false
#| error: false
library(dplyr)
library(tidyr)
dades <- dades %>%
select(-Transaction.ID, -Customer.ID, -IP.Address, -Shipping.Address,
-Billing.Address) %>%
data.frame()
reticulate::repl_python()
#| echo: false
#| warning: false
#| message: false
#| error: false
quit
#| label: preprocessing-dates
#| echo: true
#| eval: true
#| warning: false
#| message: false
#| error: false
library(janitor)
dades <- dades %>%
janitor::clean_names()
# Visualitzem el nom de les columnes
colnames(dades)
#| label: elimina-tot-NAs
#| echo: true
#| eval: true
#| warning: false
#| message: false
#| error: false
library(janitor)
# Visualitzem les dimensions de la nostra base de dades abans d'eliminar
cat("Tenim ", nrow(dades), " files i ", ncol(dades), " columnes\n")
dades <- dades %>%
janitor::remove_empty(which = c("rows", "cols"))
cat("Després de l'eliminació ens queda ", nrow(dades), " files i ", ncol(dades),
" columnes\n")
#| label: recoding
#| echo: true
#| eval: true
#| warning: false
#| message: false
#| error: false
clases <- sapply(dades, class)
varCat <- names(clases)[which(clases %in% c("character", "factor"))]
for (vC in varCat) {
dades %>%
count(get(vC)) %>% print()
}
#| label: eliminar-variables
#| echo: true
#| eval: true
#| warning: false
#| message: false
#| error: false
dades <- dades %>%
select(-transaction_date, -customer_location)
#| label: recodificar-variables
#| echo: true
#| eval: true
#| warning: false
#| message: false
#| error: false
dades <- dades %>%
mutate(is_fraudulent = case_when(
is_fraudulent == 0 ~ "no",
is_fraudulent == 1 ~ "si",
.default = is_fraudulent
))
colnames(dades)
dades <- dades %>%
mutate(is_fraudulent = case_when(
is_fraudulent == 0 ~ "no",
is_fraudulent == 1 ~ "si"
))
head(dades)
str(dades)
# Carreguem les llibreries necessaries
library(dtw)
library(tidyverse)
install.packages("dtw")
# Carreguem les llibreries necessaries
library(dtw)
#| label: carregar-dades
#| echo: true
#| eval: true
#| warning: false
#| message: false
#| error: false
url <- "https://www.ine.es/jaxiT3/files/t/es/csv_bdsc/2013.csv?nocab=1"
# Carreguem la base de dades
dades <- read.csv2(url)
dades[, 2] <- NULL
head(dades)
#| label: carregar-dades
#| echo: true
#| eval: true
#| warning: false
#| message: false
#| error: false
url <- "https://www.ine.es/jaxiT3/files/t/es/csv_bdsc/2013.csv?nocab=1"
# Carreguem la base de dades
dad <- read.csv2(url)
dad[, 2] <- NULL
#| label: carregar-llibreries
#| echo: true
#| eval: true
#| warning: false
#| message: false
#| error: false
# Carreguem les llibreries necessaries
library(dtw)
library(tidyverse)
#| label: carregar-dades
#| echo: true
#| eval: true
#| warning: false
#| message: false
#| error: false
url <- "https://www.ine.es/jaxiT3/files/t/es/csv_bdsc/2013.csv?nocab=1"
# Carreguem la base de dades
dad <- read.csv2(url)
dad[, 2] <- NULL
#| label: transformacio-dades
#| echo: true
#| eval: true
#| warning: false
#| message: false
#| error: false
# Transformem la bbdd a la taula necessaria
dad[, "Total"] <- as.numeric(dad[, "Total"])
dad[, "Zonas.Turísticas"] <- gsub(":", "_", dad[, "Zonas.Turísticas"])
dad[, "Zonas.Turísticas"] <- gsub(" ", "", dad[, "Zonas.Turísticas"])
datos <- reshape2::dcast(dad, Zonas.Turísticas ~ Periodo, fun.aggregate = sum, na.rm = TRUE)
rownames(datos) <- datos[, "Zonas.Turísticas"]
datos[, "Zonas.Turísticas"] <- NULL
head(datos)
View(datos)
#| label: calcul-distancies
#| echo: true
#| eval: true
#| warning: false
#| message: false
#| error: false
# Calculamos la distancia DTW
distMatrix <- proxy::dist(datos, method = "DTW")
#| label: crea-dendograma
#| echo: true
#| eval: true
#| warning: false
#| message: false
#| error: false
# Generamos el clustering
hcc <- hclust(distMatrix, method = "ward.D2")
plot(hcc, hang = -1, cex = 0.6) # Put the labels at the same height: hang = -1
#| label: realizamos-cortes
#| echo: true
#| eval: true
#| warning: false
#| message: false
#| error: false
# Realizamos el corte en 4 clases
k <- 4
clusters <- cutree(hcc, k = k)
#| label: visualizacion-colores
#| echo: true
#| eval: true
#| warning: false
#| message: false
#| error: false
datos %>%
proxy::dist(method = "DTW") %>%
hclust(method = "ward.D2") %>%
as.dendrogram() -> dend
dend %>% set("branches_k_color", k = k) %>% plot(hang = -1)
library(dendextend)
library(proxy)
#| label: visualizacion-colores
#| echo: true
#| eval: true
#| warning: false
#| message: false
#| error: false
datos %>%
proxy::dist(method = "DTW") %>%
hclust(method = "ward.D2") %>%
as.dendrogram() -> dend
dend %>% set("branches_k_color", k = k) %>% plot(hang = -1)
#| label: visualizacion-colores
#| echo: true
#| eval: true
#| warning: false
#| message: false
#| error: false
library(dtwclust)
install.packages("dtwclust")
#| label: libreria-automatica
#| echo: true
#| eval: true
#| warning: false
#| message: false
#| error: false
library(dtwclust)
#| label: particional-automatica
#| echo: true
#| eval: true
#| warning: false
#| message: false
#| error: false
## Partitional
pc <- tsclust(datos[, -1], type = "partitional", k = 20L,
distance = "dtw_basic", centroid = "pam",
seed = 3247L, trace = TRUE,
args = tsclust_args(dist = list(window.size = 20L)))
plot(pc)
## Partitional
pc <- tsclust(datos[, -1], type = "partitional", k = 4,
distance = "dtw_basic", centroid = "pam",
seed = 3247L, trace = TRUE,
args = tsclust_args(dist = list(window.size = 4)))
plot(pc)
# Cargamos las librerias necesarias
library(cluster)
library(fpc)
library(pracma)
library(factoextra)
library(dbscan)
install.packages("fpc")
install.packages("dbscan")
install.packages("pracma")
library(cluster)
library(fpc)
library(pracma)
library(factoextra)
library(dbscan)
#| label: generar-semilla
#| echo: true
#| eval: true
#| warning: false
#| message: false
#| error: false
### Generamos una semilla para poder ejecutar los datos
set.seed(04102022)
#| label: carregar-dades
#| echo: true
#| eval: true
#| warning: false
#| message: false
#| error: false
### Creamos la base de datos que vamos a utilizar para detectar los grupos
data("multishapes")
datos <- multishapes[, 1:2]
### Printamos la imagen que hemos obtenido de los datos a clasificar
ggplot2::ggplot(datos, aes(x = x, y = y)) +
ggplot2::geom_point(color='#3333FF')
#| label: kmeans-dades
#| echo: true
#| eval: true
#| warning: false
#| message: false
#| error: false
km_clusters <- kmeans(x = datos, centers = 5, nstart = 50)
fviz_cluster(object = km_clusters, data = datos, geom = "point", ellipse = FALSE,
show.clust.cent = FALSE, pallete = "jco") +
theme_bw() +
theme(legend.position = "none")
datos_norm
#| label: grafico-normalizacion
#| echo: true
#| eval: true
#| warning: false
#| message: false
#| error: false
ggplot2::ggplot(datos_norm, aes(x = x, y = y)) +
ggplot2::geom_point(color='#3333FF')
#| label: carregar-llibreries
#| echo: true
#| eval: true
#| warning: false
#| message: false
#| error: false
# Cargamos las librerias necesarias
library(cluster)
library(fpc)
library(pracma)
library(factoextra)
library(dbscan)
#| label: generar-semilla
#| echo: true
#| eval: true
#| warning: false
#| message: false
#| error: false
### Generamos una semilla para poder ejecutar los datos
set.seed(04102022)
#| label: carregar-dades
#| echo: true
#| eval: true
#| warning: false
#| message: false
#| error: false
### Creamos la base de datos que vamos a utilizar para detectar los grupos
data("multishapes")
datos <- multishapes[, 1:2]
### Printamos la imagen que hemos obtenido de los datos a clasificar
ggplot2::ggplot(datos, aes(x = x, y = y)) +
ggplot2::geom_point(color='#3333FF')
#| label: kmeans-dades
#| echo: true
#| eval: true
#| warning: false
#| message: false
#| error: false
km_clusters <- kmeans(x = datos, centers = 5, nstart = 50)
fviz_cluster(object = km_clusters, data = datos, geom = "point", ellipse = FALSE,
show.clust.cent = FALSE, pallete = "jco") +
theme_bw() +
theme(legend.position = "none")
#| label: dbscan
#| echo: true
#| eval: true
#| warning: false
#| message: false
#| error: false
dbscan_res <- dbscan::dbscan(datos, eps = 0.15, minPts = 5)
#| label: visualizar-dbscan
#| echo: true
#| eval: true
#| warning: false
#| message: false
#| error: false
fviz_cluster(object = dbscan_res, data = datos, geom = "point", ellipse = FALSE,
show.clust.cent = FALSE, pallete = "jco") +
theme_bw() +
theme(legend.position = "none")
#| label: calculo-minPts
#| echo: true
#| eval: true
#| warning: false
#| message: false
#| error: false
porcentaje <- 0.0025
# Cálculo de min_pts.
min_pts <- round(nrow(datos) * porcentaje)
min_pts
# Realizamos los cortes de 2 y 10 que se mencionan anteriormente como validación
# adicional, pero lineas 98 y 99 pueden comentarse.
min_pts <- ifelse(min_pts <= 1, 2, min_pts)
min_pts <- ifelse(min_pts >= 10, 10, min_pts)
#| label: normalizacion
#| echo: true
#| eval: true
#| warning: false
#| message: false
#| error: false
datos_norm <- data.frame(lapply(datos, scales::rescale))
head(datos_norm)
#| label: grafico-normalizacion
#| echo: true
#| eval: true
#| warning: false
#| message: false
#| error: false
ggplot2::ggplot(datos_norm, aes(x = x, y = y)) +
ggplot2::geom_point(color='#3333FF')
#| label: vecindad-distancias
#| echo: true
#| eval: false
#| warning: false
#| message: false
#| error: false
distanciasVecinas <- dbscan::kNNdist(datos, k = min_pts)
distanciasVecinas
Y <- distanciasVecinas[order(distanciasVecinas)]
Y
X <- c(0:(length(Y) - 1))
X
pendientes <- c()
for (i in 1:length(X)) {
pendientes[i] <- (Y[i + 1] - Y[i])/(X[i+1] - X[i])
}
m <- which.max(pendientes)
primer <- gdata::first(which(pendientes >= m))
install.packages("gdata")
m
primer <- gdata::first(which(pendientes >= m))
primer
pendientes >= m
m <- which.max(pendientes)
primer <- gdata::first(which(pendientes >= m))
primer
epsilon <- Y[primer]
epsilon
kNNdistplot(datos, k = 5, minPts = min_pts)
abline(h = 0.15, lty = 2, col = "red")
#| label: graficar-epsilon
#| echo: true
#| eval: true
#| warning: false
#| message: false
#| error: false
{kNNdistplot(datos, k = 5, minPts = min_pts)
abline(h = 0.15, lty = 2, col = "red")}
res <- dbscan(datos, eps = epsilon, minPts = min_pts)
epsilon
epsilon <- 0.15
min_pts
epsilon
datos
res <- dbscan(datos, eps = epsilon, minPts = min_pts)
res
install.packages("labeleR")
install.packages("remotes")
remotes::install_github("Pakillo/CityShadeMapper")
library(terra)
library(CityShadeMapper)
library(lidR)
# Descargar datos de lidr (P...)
lidr <- lidR::readLAS("C:/Users/Sergi/OneDrive/Escritorio/PNOA_2008_Lote_CAT_430-4584_ORT-CLA-COL.laz")
lidr
plot(lidr)
alturas <- calc_heights_from_lidar(lidr)
plot(alturas)
# Calcular mapa de sombras
sombra <- make_shademap(alturas, data = "2025-01-16", hour = "15")
# Calcular mapa de sombras
sombra <- make_shademap(alturas, date = "2025-01-16", hour = "15")
# Calcular mapa de sombras
sombra <- make_shademap(alturas, date = "2025-01-16", hour = 15)
sombra
plot(sombra)
plot_shademap(sombra)
plot_shademap(sombra, animate = TRUE, maxcell = 400000)
url = "C:/Users/Sergi/OneDrive/Escritorio/PNOA_2008_Lote_CAT_430-4584_ORT-CLA-COL.laz"
## Primero estimar coberturas
cover <- rasterize_lidar_cover_class(url)
cover
plot(cover)
sombra.suelo <- make_shademap(alturas, data = "2024-12-24", hour = 15,
type = "ground", cover.ras = cover)
sombra.suelo <- make_shademap(alturas, date = "2024-12-24", hour = 15,
type = "ground", cover.ras = cover)
sombra.suelo
plot(sombra.suelo)
plot(sombra.suelo)
